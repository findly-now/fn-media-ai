name: CI - Build, Test, and Quality Checks

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]

# Global environment variables
env:
  PYTHON_VERSION: '3.11'
  POETRY_VERSION: '1.6.1'
  REGISTRY: ghcr.io
  IMAGE_NAME: fn-media-ai

# Limit concurrency to one workflow per branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # =================================================================
  # Code Quality and Security Checks
  # =================================================================
  quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Code formatting check (Black)
      run: |
        black --check --diff src/ tests/
        if [ $? -ne 0 ]; then
          echo "âŒ Code formatting issues found. Run 'make format' to fix."
          exit 1
        fi

    - name: Import sorting check (isort)
      run: |
        isort --check-only --diff src/ tests/
        if [ $? -ne 0 ]; then
          echo "âŒ Import sorting issues found. Run 'make format' to fix."
          exit 1
        fi

    - name: Linting (flake8)
      run: |
        flake8 src/ tests/ --statistics
        echo "âœ… Linting completed"

    - name: Type checking (mypy)
      run: |
        mypy src/
        echo "âœ… Type checking completed"

    - name: Security analysis (bandit)
      run: |
        bandit -r src/ -f json -o security-report.json
        bandit -r src/ -f txt
        echo "âœ… Security analysis completed"

    - name: Upload security report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: security-report.json

  # =================================================================
  # Dependency and Vulnerability Scanning
  # =================================================================
  security:
    name: Dependency Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety pip-audit

    - name: Check for known vulnerabilities (Safety)
      run: |
        safety check --json --output safety-report.json || true
        safety check || echo "âš ï¸ Security vulnerabilities found (see report)"

    - name: Audit dependencies (pip-audit)
      run: |
        pip-audit --format=json --output=pip-audit-report.json || true
        pip-audit || echo "âš ï¸ Audit issues found (see report)"

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: dependency-security-reports
        path: |
          safety-report.json
          pip-audit-report.json

  # =================================================================
  # Unit and Integration Tests
  # =================================================================
  test:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      # Redis for caching tests
      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

      # PostgreSQL for database tests
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_findly_media_ai
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U test_user -d test_findly_media_ai"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          tesseract-ocr \
          tesseract-ocr-eng \
          libopencv-dev \
          postgresql-client

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Set up test environment
      env:
        REDIS_URL: redis://localhost:6379/0
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_findly_media_ai
        ENVIRONMENT: test
        LOG_LEVEL: DEBUG
        TEST_MOCK_EXTERNAL_APIS: true
      run: |
        # Create test database schema
        psql $DATABASE_URL -c "CREATE EXTENSION IF NOT EXISTS postgis;"

        # Verify Redis connection
        redis-cli ping

        echo "âœ… Test environment ready"

    - name: Run E2E tests with coverage
      env:
        REDIS_URL: redis://localhost:6379/0
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_findly_media_ai
        ENVIRONMENT: test
        LOG_LEVEL: DEBUG
        TEST_MOCK_EXTERNAL_APIS: true
        OPENAI_API_KEY: test-key
        KAFKA_BROKERS: localhost:9092
      run: |
        pytest tests/e2e/ \
          -v \
          --cov=src/fn_media_ai \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junitxml=test-results.xml \
          --maxfail=5 \
          --tb=short

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: |
          test-results.xml
          htmlcov/
          .coverage

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: success()
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  # =================================================================
  # Docker Build and Security Scan
  # =================================================================
  docker:
    name: Docker Build & Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [quality, test]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image (production)
      uses: docker/build-push-action@v5
      with:
        context: .
        target: production
        tags: ${{ env.IMAGE_NAME }}:test
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build Docker image (GPU)
      uses: docker/build-push-action@v5
      with:
        context: .
        target: gpu
        tags: ${{ env.IMAGE_NAME }}:test-gpu
        load: true
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Test Docker image
      run: |
        # Test production image
        docker run --rm -d --name test-container -p 8000:8000 \
          -e ENVIRONMENT=test \
          -e LOG_LEVEL=DEBUG \
          -e TEST_MOCK_EXTERNAL_APIS=true \
          ${{ env.IMAGE_NAME }}:test

        # Wait for container to start
        sleep 30

        # Test health endpoint
        curl -f http://localhost:8000/health || exit 1

        # Clean up
        docker stop test-container

    - name: Run Trivy security scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.IMAGE_NAME }}:test
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run Hadolint (Dockerfile linter)
      uses: hadolint/hadolint-action@v3.1.0
      with:
        dockerfile: Dockerfile
        format: sarif
        output-file: hadolint-results.sarif

    - name: Upload Hadolint results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: hadolint-results.sarif

  # =================================================================
  # AI Model Validation
  # =================================================================
  ai-models:
    name: AI Model Validation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Download and validate AI models
      env:
        HUGGINGFACE_HUB_CACHE: /tmp/hf_cache
      run: |
        mkdir -p /tmp/hf_cache

        # Test model downloads
        python -c "
        try:
            from transformers import AutoModel
            print('âœ… Transformers available')
            # Test small model download
            model = AutoModel.from_pretrained('microsoft/DialoGPT-small', cache_dir='/tmp/hf_cache')
            print('âœ… HuggingFace model download successful')
        except Exception as e:
            print(f'âŒ Model download failed: {e}')
            exit(1)

        try:
            from ultralytics import YOLO
            print('âœ… YOLO available')
            # Test YOLO model
            model = YOLO('yolov8n.pt')
            print('âœ… YOLO model download successful')
        except Exception as e:
            print(f'âŒ YOLO download failed: {e}')
            exit(1)
        "

  # =================================================================
  # Performance Benchmarks
  # =================================================================
  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark

    - name: Run performance benchmarks
      env:
        ENVIRONMENT: test
        TEST_MOCK_EXTERNAL_APIS: true
      run: |
        pytest tests/e2e/ \
          -k "benchmark" \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          || echo "âš ï¸ No benchmark tests found"

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: benchmark-results.json

  # =================================================================
  # Summary Report
  # =================================================================
  summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [quality, security, test, docker]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate CI summary
      run: |
        echo "# ðŸ¤– FN Media AI - CI Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Quality status
        if [ "${{ needs.quality.result }}" == "success" ]; then
          echo "âœ… **Code Quality**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Code Quality**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        # Security status
        if [ "${{ needs.security.result }}" == "success" ]; then
          echo "âœ… **Security Scan**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Security Scan**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        # Test status
        if [ "${{ needs.test.result }}" == "success" ]; then
          echo "âœ… **E2E Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **E2E Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        # Docker status
        if [ "${{ needs.docker.result }}" == "success" ]; then
          echo "âœ… **Docker Build**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Docker Build**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Workflow**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY